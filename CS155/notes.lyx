#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
CS 155 Machine Learning and Data Mining
\end_layout

\begin_layout Part*
Tuesday, January 5
\end_layout

\begin_layout Standard
Yay all lectures will be recorded.
 
\end_layout

\begin_layout Standard
Recitation is Thursday 7:30 to 9.
 
\end_layout

\begin_layout Standard
six assignments.
 2 mini projects and a final.
 so 9 thing that range from 10% to 16% of the grade.
\end_layout

\begin_layout Standard
With CS 156, the assignment should take 3-4 hours
\end_layout

\begin_layout Standard
Without then might take up 8-10 hours.
 but should catch you up.
\end_layout

\begin_layout Standard
Process of converting data and experience into knowledge.
 lots and lots of data into a computer model via some algorithm
\end_layout

\begin_layout Standard
ML is about algorithms and DM is about the knowledge extraction.
 want to make it human-understandable.
\end_layout

\begin_layout Standard
classifying emails by spam or not spam.
 can't just use if blocks.
 easy to recognize but hard t owrite down formula to describe which is spam
 and which isn't
\end_layout

\begin_layout Standard
use ML, we create a training set sorted by humans.
 then we create a generic representation and a learning algorithm so we
 can then classify properly.
 
\end_layout

\begin_layout Standard
we can use a bag of words represtnation.
 each email is a vector of numbers.
 it counts how many of each word in the dictionary.
 or can be booleans -- if that particular word is in the document.
 called a Feature Vector.
 
\end_layout

\begin_layout Standard
Linear models -- x is a bag of words representation of an email.
 linear classifier has a weight =vector and a bias so we multiple the weights
 by the words (0 or 1) and subtract a bias.
 then check the sign.
 want to make it so that we can perfectly classify the training set.
 then we guess it probably works elsewhere
\end_layout

\begin_layout Standard
there will be high weight on Nigerian Prince.
 ie it is usually spam.
 
\end_layout

\begin_layout Standard
2 ML problems: classification (which class does an object belong to
\begin_inset Formula $sign(w^{T}x-b)$
\end_inset

) and regression (predict a real value, so what is the confidence that something
 is spam 
\begin_inset Formula $w^{T}x-b$
\end_inset

.
 Now we have to care about false positives and negatives and things.
 
\end_layout

\begin_layout Subsubsection*
Formal Definitions:
\end_layout

\begin_layout Standard
Training Set 
\begin_inset Formula $S=\{(x_{i},y_{i})\}_{i=1}^{N}$
\end_inset


\end_layout

\begin_layout Standard
Model Class 
\begin_inset Formula $f(x|w;b)=w^{T}x-b$
\end_inset


\end_layout

\begin_layout Standard
Loss function (squared loss) 
\begin_inset Formula $L(a,b)=(a-b)^{2}$
\end_inset


\end_layout

\begin_layout Standard
Learning Objective minimize loss
\end_layout

\begin_layout Standard
Different loss functions: zero one loss -- all or nothing, just an indicator
 function.
 
\end_layout

\begin_layout Standard
squared loss is an upper bound on zero-one loss
\end_layout

\begin_layout Standard
zero one loss is discontinutous.
 can't compute the gradient there.
 very difficult to optimize.
 so we want to use squared loss becausethe gradient IS defined properly.
\end_layout

\begin_layout Standard
need to pull the training set peroperly from the true distribution.
\end_layout

\begin_layout Standard
overfitting is when test error is much larger than training error
\end_layout

\begin_layout Standard
variance/bias tradeoff is part of overfitting.
 
\end_layout

\begin_layout Standard
high variaince implies underfitting and high bias is ovrefittting
\end_layout

\begin_layout Standard
cross validation is the idea od trying out the differetn models and testing
 on the last bit of traiing data rather than training on it.
 then we chose the best model
\end_layout

\begin_layout Part*
Thursday, January 7
\end_layout

\begin_layout Subsection
Perceptron
\end_layout

\begin_layout Standard
early powerful ML approach
\end_layout

\begin_layout Standard
fast, clean and like neural networks
\end_layout

\begin_layout Standard
start with a model, chose random values from training set.
 if our model would work ie classifies correctly, we leave it alone, otherwise
 we make it different to be closer to the one that we had as an issue.
 
\end_layout

\begin_layout Standard
a hyperplane (line in the 2d case) it is describd by 
\begin_inset Formula $(w,b)$
\end_inset

where 
\begin_inset Formula $w$
\end_inset

 is 
\begin_inset Formula $d$
\end_inset

 dimensional and 
\begin_inset Formula $b$
\end_inset

 is the scalar bias.
 
\end_layout

\begin_layout Standard
\begin_inset Formula $distance=\frac{|w^{T}x-b|}{||w||}$
\end_inset

 and the signed distance is where we remove the absolute value.if we didn't
 normalize, we would have the linear mode, ie 
\begin_inset Formula $w^{T}x-b$
\end_inset


\end_layout

\begin_layout Standard
example (with bias ignored)
\end_layout

\begin_layout Standard
we win when no more updates are made -- so we could just look through all
 of the training data and know we are done.
 thoughits also m=possible that we get oscillations and never terminate.
 
\end_layout

\begin_layout Standard
serperating hyperplane.
 all of the correct ways to divide the two groups form a seperating hyperplane.
 
\end_layout

\begin_layout Standard
a training set is linearly seperable only if there exists a hyperplane that
 is mistake free between the two groups
\end_layout

\begin_layout Standard
\begin_inset Formula $\gamma=margin=\mbox{\frac{y(w^{T}x)}{|w|}}$
\end_inset

 we want to maximize the minimum distance.
 
\end_layout

\begin_layout Standard
linearly seperable iff 
\begin_inset Formula $\gamma>0$
\end_inset


\end_layout

\begin_layout Standard
If the problem is linearly seperable, then the number of mistakes is bounded
 by 
\begin_inset Formula $R^{2}/\gamma^{2}$
\end_inset

 where 
\begin_inset Formula $R=max||x||$
\end_inset

 and 
\begin_inset Formula $\gamma$
\end_inset

 is the margin.
 More compact representation fewer mistakes.
 larger margin implies fewer mistakes.
\end_layout

\begin_layout Standard
Most problems are not actually linearly seperable, so the perceptron might
 never converge.
 one way to avoid this problem is to keep comparing proposed functions on
 a validation set.
 pick the best function defined by how well it did on the validation set.
 that or stop continuing after accuracy starts to drop.
 called early stopping.
 can also stop after a fixed number of passes and choose best out of those.
 
\end_layout

\begin_layout Standard
online learning vs batch learning
\end_layout

\begin_layout Standard
online learning is defined by getting a stream of data, not all at once.
 can only modify and update.
 perceptron is an example
\end_layout

\begin_layout Standard
batch learning says that we have all of the data at once.
 then we make a single update based on all of the data simaltaneously
\end_layout

\begin_layout Standard
can always use online learning on batch learning problem because we can
 stream the data.
 Can't do the reverse typically.
 
\end_layout

\begin_layout Standard
However, its not always a good model and sometimes doesn't even converge.
 
\end_layout

\begin_layout Subsection
Gradient Descent
\end_layout

\begin_layout Standard
optimizing objective functions
\end_layout

\begin_layout Standard
want to minimize the loss function.
 
\end_layout

\begin_layout Standard
gradient of loss function.
 looks just like the HW problem.
 its on the slides
\end_layout

\begin_layout Standard
if we leave the step size 
\begin_inset Formula $\eta$
\end_inset

 the same, then we can oscillate infinitely.
 if we make 
\begin_inset Formula $\eta$
\end_inset

 much smaller it will take a while, but we can make the bounds much closer
 before oscillation takes over.
\end_layout

\begin_layout Standard
so we want the step size to be as large as possible without causing divergence.
 
\end_layout

\begin_layout Standard
if we have the same optimization problem but scale the loss function up
 by a constant factor.
 
\end_layout

\begin_layout Standard
divide the loss function by the number of examples.
 then we can have step sizes on the same scale as other problems.
 start with large step size and once the loss plateus, divide the step size
 by some other nubmer.
 
\series bold
in order to get to the global optimum, the step size must change
\series default
.
 
\end_layout

\begin_layout Standard
convex functions.
 easy to find their global minimum bc every local minimum is global.
\end_layout

\begin_layout Standard
download new copies of the slide beause there will be a typo update.
 
\end_layout

\begin_layout Standard
how fast can we get within 
\begin_inset Formula $\epsilon$
\end_inset

 of the optimal loss? 
\end_layout

\begin_layout Standard
some of these functions can be super fast.
 
\begin_inset Formula $O(1/\epsilon^{2})$
\end_inset

is the worst one.
 but the fast ones are 
\begin_inset Formula $O(log(1/\epsilon))$
\end_inset


\end_layout

\begin_layout Standard
there are a number of stopping strategies.
 when validation error stops going down, after a number of iterations, after
 a certain about of time etc.
\end_layout

\begin_layout Standard
the problem with gradient descent is that it requires summing over the entire
 training set every time which is super expensive.
 we need to page in data from HDD because you sohuld have more data than
 RAM.
 so maybe we only need to pass over part of the data.
 
\end_layout

\begin_layout Subsubsection*
Stochastic Gradient Descent
\end_layout

\begin_layout Standard
we want to avoid passing over everything because that's too much.
\end_layout

\begin_layout Standard
what if the loss function decomposes additively.
 
\end_layout

\begin_layout Standard
mini-batch SGD.
 get a component function per batch of training examples.
 this let's us use vectors to do tihngs faster, and it has less effect on
 the function every time so it decreases volitility.
 is also pretty parrallelizeable.
 
\end_layout

\begin_layout Standard
must be divided up randomly.
 and need a data set large enough.
 doing the batching differently will affect the answer, but that answer
 will still be in the 
\begin_inset Formula $\epsilon$
\end_inset

ball.
\end_layout

\begin_layout Standard
the loss function for a single training example.
 only update when the sign of the label is wrong.
 perceptron algorithm implicitly does SGD with this particular loss function
 and step size of one.
\end_layout

\end_body
\end_document
